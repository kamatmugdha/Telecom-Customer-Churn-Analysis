{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YApqWcigj8Nv"
   },
   "outputs": [],
   "source": [
    "'''# innstall java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "# install findspark using pip\n",
    "!pip install -q findspark'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Tn6ohxS-l71y"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX7VXkgHom4a",
    "outputId": "e31c9f3b-d087-433c-9998-5d85e91e425e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MONTANT: double (nullable = true)\n",
      " |-- FREQUENCE_RECH: double (nullable = true)\n",
      " |-- ARPU_SEGMENT: double (nullable = true)\n",
      " |-- FREQUENCE: double (nullable = true)\n",
      " |-- DATA_VOLUME: double (nullable = true)\n",
      " |-- ON_NET: double (nullable = true)\n",
      " |-- ORANGE: double (nullable = true)\n",
      " |-- TIGO: double (nullable = true)\n",
      " |-- REGULARITY: double (nullable = true)\n",
      " |-- FREQ_TOP_PACK: double (nullable = true)\n",
      " |-- CHURN: integer (nullable = true)\n",
      " |-- REGION_DIOURBEL: integer (nullable = true)\n",
      " |-- REGION_FATICK: integer (nullable = true)\n",
      " |-- REGION_KAFFRINE: integer (nullable = true)\n",
      " |-- REGION_KAOLACK: integer (nullable = true)\n",
      " |-- REGION_KEDOUGOU: integer (nullable = true)\n",
      " |-- REGION_KOLDA: integer (nullable = true)\n",
      " |-- REGION_LOUGA: integer (nullable = true)\n",
      " |-- REGION_MATAM: integer (nullable = true)\n",
      " |-- REGION_Other: integer (nullable = true)\n",
      " |-- REGION_SAINT-LOUIS: integer (nullable = true)\n",
      " |-- REGION_SEDHIOU: integer (nullable = true)\n",
      " |-- REGION_TAMBACOUNDA: integer (nullable = true)\n",
      " |-- REGION_THIES: integer (nullable = true)\n",
      " |-- REGION_ZIGUINCHOR: integer (nullable = true)\n",
      " |-- TENURE_E 6-9 month: integer (nullable = true)\n",
      " |-- TENURE_F 9-12 month: integer (nullable = true)\n",
      " |-- TENURE_G 12-15 month: integer (nullable = true)\n",
      " |-- TENURE_H 15-18 month: integer (nullable = true)\n",
      " |-- TENURE_I 18-21 month: integer (nullable = true)\n",
      " |-- TENURE_J 21-24 month: integer (nullable = true)\n",
      " |-- TENURE_K > 24 month: integer (nullable = true)\n",
      " |-- TOP_PACK_1500=Unlimited7Day: integer (nullable = true)\n",
      " |-- TOP_PACK_200=Unlimited1Day: integer (nullable = true)\n",
      " |-- TOP_PACK_200F=10mnOnNetValid1H: integer (nullable = true)\n",
      " |-- TOP_PACK_305155009: integer (nullable = true)\n",
      " |-- TOP_PACK_500=Unlimited3Day: integer (nullable = true)\n",
      " |-- TOP_PACK_APANews_weekly: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 1000=5000;5d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 1000F=(3000F On+3000F Off);5d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 300=600;2d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 5000= 20000off+20000on;30d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 500= 4000off+4000on;24H: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 500F =2000F_AllNet_Unlimited: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 500F=1250F_AllNet_1250_Onnet;48h: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 500F=2000F;5d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 500F=4000F ; 5d: integer (nullable = true)\n",
      " |-- TOP_PACK_All-net 600F= 3000F ;5d: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_100F_unlimited: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_100f=200 MB: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_200f=400MB: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_500f=2GB: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_On-net 400f=2200F: integer (nullable = true)\n",
      " |-- TOP_PACK_CVM_on-net bundle 500=5000: integer (nullable = true)\n",
      " |-- TOP_PACK_Data: 100 F=40MB,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data: 200 F=100MB,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data: 200F=1GB,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data: 490F=Night,00H-08H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:1000F=2GB,30d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:1000F=5GB,7d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:1500F=3GB,30D: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:1500F=SPPackage1,30d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:150F=SPPackage1,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:200F=Unlimited,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:3000F=10GB,30d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:300F=100MB,2d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:30Go_V 30_Days: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:490F=1GB,7d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:500F=2GB,24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:50F=30MB_24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:700F=1.5GB,7d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:700F=SPPackage1,7d: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:DailyCycle_Pilot_1.5GB: integer (nullable = true)\n",
      " |-- TOP_PACK_Data:OneTime_Pilot_1.5GB: integer (nullable = true)\n",
      " |-- TOP_PACK_DataPack_Incoming: integer (nullable = true)\n",
      " |-- TOP_PACK_Data_EVC_2Go24H: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_100Mo: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_1Go: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_4900=12000F: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_500=2000F: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_700Mo: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_JOKKO30: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_Jokko_Weekly: integer (nullable = true)\n",
      " |-- TOP_PACK_EVC_MEGA10000F: integer (nullable = true)\n",
      " |-- TOP_PACK_FIFA_TS_daily: integer (nullable = true)\n",
      " |-- TOP_PACK_FNF2 ( JAPPANTE): integer (nullable = true)\n",
      " |-- TOP_PACK_FNF_Youth_ESN: integer (nullable = true)\n",
      " |-- TOP_PACK_Facebook_MIX_2D: integer (nullable = true)\n",
      " |-- TOP_PACK_Go-NetPro-4 Go: integer (nullable = true)\n",
      " |-- TOP_PACK_IVR Echat_Daily_50F: integer (nullable = true)\n",
      " |-- TOP_PACK_IVR Echat_Weekly_200F: integer (nullable = true)\n",
      " |-- TOP_PACK_Incoming_Bonus_woma: integer (nullable = true)\n",
      " |-- TOP_PACK_Internat: 1000F_Zone_1;24H\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_Internat: 1000F_Zone_3;24h\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_Internat: 2000F_Zone_2;24H\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_Jokko_Daily: integer (nullable = true)\n",
      " |-- TOP_PACK_Jokko_Monthly: integer (nullable = true)\n",
      " |-- TOP_PACK_Jokko_Weekly: integer (nullable = true)\n",
      " |-- TOP_PACK_Jokko_promo: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 200mnoff net _unl on net _5Go;30d: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 390F=04HOn-net_400SMS_400 Mo;4h\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 4900F= 10H on net_1,5Go ;30d: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 5000F=80Konnet_20Koffnet_250Mo;30d\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 500F=75(SMS, ONNET, Mo)_1000FAllNet;24h\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT: 590F=02H_On-net_200SMS_200 Mo;24h\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT:10000F=10hAllnet_3Go_1h_Zone3;30d\\t\\t: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT:1000F=4250 Off net _ 4250F On net _100Mo; 5d: integer (nullable = true)\n",
      " |-- TOP_PACK_MIXT:500F= 2500F on net _2500F off net;2d: integer (nullable = true)\n",
      " |-- TOP_PACK_MROMO_TIMWES_OneDAY: integer (nullable = true)\n",
      " |-- TOP_PACK_MROMO_TIMWES_RENEW: integer (nullable = true)\n",
      " |-- TOP_PACK_Mixt 250F=Unlimited_call24H: integer (nullable = true)\n",
      " |-- TOP_PACK_Mixt : 500F=2500Fonnet_2500Foffnet ;5d: integer (nullable = true)\n",
      " |-- TOP_PACK_NEW_CLIR_PERMANENT_LIBERTE_MOBILE: integer (nullable = true)\n",
      " |-- TOP_PACK_New_YAKALMA_4_ALL: integer (nullable = true)\n",
      " |-- TOP_PACK_No Top pack: integer (nullable = true)\n",
      " |-- TOP_PACK_On net 200F= 3000F_10Mo ;24H: integer (nullable = true)\n",
      " |-- TOP_PACK_On net 200F=Unlimited _call24H: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 1000F=10MilF;10d: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 2000f_One_Month_100H; 30d: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 200F=60mn;1d: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 300F=1800F;3d: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 500=4000,10d: integer (nullable = true)\n",
      " |-- TOP_PACK_On-net 500F_FNF;3d: integer (nullable = true)\n",
      " |-- TOP_PACK_Pilot_Youth1_290: integer (nullable = true)\n",
      " |-- TOP_PACK_Pilot_Youth4_490: integer (nullable = true)\n",
      " |-- TOP_PACK_SUPERMAGIK_1000: integer (nullable = true)\n",
      " |-- TOP_PACK_SUPERMAGIK_5000: integer (nullable = true)\n",
      " |-- TOP_PACK_TelmunCRBT_daily: integer (nullable = true)\n",
      " |-- TOP_PACK_Twter_U2opia_Daily: integer (nullable = true)\n",
      " |-- TOP_PACK_Twter_U2opia_Monthly: integer (nullable = true)\n",
      " |-- TOP_PACK_Twter_U2opia_Weekly: integer (nullable = true)\n",
      " |-- TOP_PACK_VAS(IVR_Radio_Daily): integer (nullable = true)\n",
      " |-- TOP_PACK_VAS(IVR_Radio_Monthly): integer (nullable = true)\n",
      " |-- TOP_PACK_VAS(IVR_Radio_Weekly): integer (nullable = true)\n",
      " |-- TOP_PACK_YMGX 100=1 hour FNF, 24H/1 month: integer (nullable = true)\n",
      " |-- TOP_PACK_Yewouleen_PKG: integer (nullable = true)\n",
      " |-- TOP_PACK_pack_chinguitel_24h: integer (nullable = true)\n",
      " |-- TOP_PACK_pilot_offer7: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# start a spark session \n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"7g\").getOrCreate()\n",
    "\n",
    "# load data with inferred schema \n",
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\"Complete_Data_BDT.csv\")\n",
    "#df = spark.read.load(\"C:/Users/Admin/Downloads/Complete_Data_BDT.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# The inferred schema can be seen using .printSchema().\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wDkTGoCJmzoE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Converting spark df into pandas just for using SMOTE\n",
    "df = df.toPandas()\n",
    "\n",
    "#splitting X and Y sets and the test train data\n",
    "y = df.CHURN\n",
    "x = df.drop(columns=['CHURN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fP33lTShnJc8"
   },
   "outputs": [],
   "source": [
    "#splitting test and train sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00CUW9F70rCT"
   },
   "source": [
    "Upsample is done only for train dataset and test should be untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fuXMsZCnS5G",
    "outputId": "896375e0-cb63-469c-a089-ef002ff7ea6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  837618\n",
      "Number of no subscription in oversampled data 418809\n",
      "Number of subscription 418809\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  0.5\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "#columns = X_train.columns\n",
    "os_data_x,os_data_y=os.fit_resample(x_train, y_train)\n",
    "x_train = pd.DataFrame(data=os_data_x, )\n",
    "y_train = pd.DataFrame(data=os_data_y,columns=['CHURN'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(x_train))\n",
    "print(\"Number of no subscription in oversampled data\",len(y_train[y_train['CHURN']==0]))\n",
    "print(\"Number of subscription\",len(y_train[y_train['CHURN']==1]))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(y_train[y_train['CHURN']==0])/len(x_train))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(y_train[y_train['CHURN']==1])/len(x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data is  107246\n"
     ]
    }
   ],
   "source": [
    "print(\"length of test data is \",len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PeL2KaHt0fT"
   },
   "source": [
    "Now the pandas df is converted back to spark df for further modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odAyPAUUwmGP",
    "outputId": "16d9cbfd-49e8-4fe1-a6b2-6dc4ba8d36e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 837618 entries, 0 to 837617\n",
      "Columns: 138 entries, MONTANT to CHURN\n",
      "dtypes: float64(10), int32(128)\n",
      "memory usage: 472.9 MB\n"
     ]
    }
   ],
   "source": [
    "combined_train_dataset = pd.concat([x_train,y_train], axis=1, join='inner')\n",
    "combined_train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2LEa25p0ft-",
    "outputId": "6848bd00-f305-434a-d324-f2e160433094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 107246 entries, 236859 to 110351\n",
      "Columns: 138 entries, MONTANT to CHURN\n",
      "dtypes: float64(10), int32(128)\n",
      "memory usage: 61.4 MB\n"
     ]
    }
   ],
   "source": [
    "combined_test_dataset = pd.concat([x_test,y_test], axis=1, join='inner')\n",
    "combined_test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bM72ZN8ItYnp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\pandas\\conversion.py:329: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#converting to spark df\n",
    "train_dataset = spark.createDataFrame(combined_train_dataset)\n",
    "test_dataset = spark.createDataFrame(combined_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgpiEZ2tCR3H"
   },
   "source": [
    "Now createing feature vector using vectorAssemebler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YGDRg5l3Dt_s"
   },
   "outputs": [],
   "source": [
    "#vector assembler threw error since one column header contains '.' . removing column headers before passing it to the function\n",
    "tempList = [] #Edit01\n",
    "for col in train_dataset.columns:\n",
    "  new_name = col.strip()\n",
    "  #new_name = \"\".join(new_name.split())\n",
    "  new_name = new_name.replace('.','point') # EDIT\n",
    "  tempList.append(new_name) #Edit02\n",
    "#print(tempList) #Just for the sake of it #Edit03\n",
    "\n",
    "train_dataset = train_dataset.toDF(*tempList)\n",
    "test_dataset = test_dataset.toDF(*tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LDBGqCQOCRFm"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = train_dataset.drop('CHURN').columns\n",
    "outputCol = \"features\"\n",
    "assembler = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "df_va_train = assembler.transform(train_dataset)\n",
    "\n",
    "df_va_test = assembler.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfgUjZLQpb1z"
   },
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EQ1IEodYom4l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Decision Tree Classifier : 46.843886852264404\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"CHURN\", featuresCol=\"features\", maxDepth=25, minInstancesPerNode=30, impurity=\"gini\")\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "model = pipeline.fit(df_va_train)\n",
    "\n",
    "print(\"Time Taken for Decision Tree Classifier : \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gdeRoTOBom4l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.transform(df_va_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "-HmcmtGoom4s"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"CHURN\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "precision = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"precisionByLabel\"})\n",
    "recall = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"recallByLabel\"})\n",
    "f1 = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"f1\"})\n",
    "roc_auc = evaluator.evaluate(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4Hc1mvbom4s",
    "outputId": "5d32b31b-ad09-41ea-89e4-0d46d7dfb5b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.906803, precision: 0.989045, recall: 0.914606, f1: 0.932991, roc_auc: 0.605211\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLSfvZ0WtVHB"
   },
   "source": [
    "Gradient Boosting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1rC6yhnrom4t"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "# define the classifier \n",
    "gbt = GBTClassifier(labelCol=\"CHURN\", featuresCol=\"features\")\n",
    "\n",
    "# train the classifier with train data \n",
    "GBT = gbt.fit(df_va_train)\n",
    "\n",
    "# predict test data \n",
    "y_pred_GBT = GBT.transform(df_va_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DOvHxnsCwKoJ"
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"CHURN\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(y_pred_GBT, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "precision = evaluatorMulti.evaluate(y_pred_GBT, {evaluatorMulti.metricName: \"precisionByLabel\"})\n",
    "recall = evaluatorMulti.evaluate(y_pred_GBT, {evaluatorMulti.metricName: \"recallByLabel\"})\n",
    "f1 = evaluatorMulti.evaluate(y_pred_GBT, {evaluatorMulti.metricName: \"f1\"})\n",
    "roc_auc = evaluator.evaluate(y_pred_GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMaPIqfcw2kq",
    "outputId": "9d554ba4-9103-498f-9f86-1e04fc258778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.878858, precision: 0.995437, recall: 0.879866, f1: 0.917477, roc_auc: 0.930415\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE--y_CSwjXW"
   },
   "source": [
    "Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fxz4TQ9Qom4t"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"CHURN\",featuresCol=\"features\").setImpurity(\"gini\").setMaxDepth(6).setSeed(90)\n",
    "\n",
    "RFC = rf.fit(df_va_train)\n",
    "\n",
    "y_pred_RF = RFC.transform(df_va_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "5BUbt2qNom4u"
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"CHURN\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(y_pred_RF, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "precision = evaluatorMulti.evaluate(y_pred_RF, {evaluatorMulti.metricName: \"precisionByLabel\"})\n",
    "recall = evaluatorMulti.evaluate(y_pred_RF, {evaluatorMulti.metricName: \"recallByLabel\"})\n",
    "f1 = evaluatorMulti.evaluate(y_pred_RF, {evaluatorMulti.metricName: \"f1\"})\n",
    "roc_auc = evaluator.evaluate(y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqDWYIbfom4u",
    "outputId": "47f26392-6d6e-4a8d-a718-f93fc58bbf2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.859575, precision: 0.996849, recall: 0.858783, f1: 0.905955, roc_auc: 0.929747\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i3qkirlx5Pk"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "eu3lx8Z_om4n"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression\n",
    "\n",
    "# define the classifier \n",
    "log_model = LogisticRegression(labelCol=\"CHURN\", featuresCol=\"features\", maxIter=1000)\n",
    "\n",
    "# train the classifier with train data \n",
    "LG = log_model.fit(df_va_train)\n",
    "\n",
    "# predict test data \n",
    "y_pred_LR = LG.transform(df_va_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Am3GwWh-yUYJ"
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"CHURN\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(y_pred_LR, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "precision = evaluatorMulti.evaluate(y_pred_LR, {evaluatorMulti.metricName: \"precisionByLabel\"})\n",
    "recall = evaluatorMulti.evaluate(y_pred_LR, {evaluatorMulti.metricName: \"recallByLabel\"})\n",
    "f1 = evaluatorMulti.evaluate(y_pred_LR, {evaluatorMulti.metricName: \"f1\"})\n",
    "roc_auc = evaluator.evaluate(y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCpsglOAykK4",
    "outputId": "8fa3efa2-49ad-44e4-ef7d-5222da2a592f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.871053, precision: 0.996403, recall: 0.870978, f1: 0.912881, roc_auc: 0.934114\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BDT_Proj_OnlyAlgos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
